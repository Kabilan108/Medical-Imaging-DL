{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dermnet: Inception V3\n",
    "\n",
    "Here I will use the Inception V3 model to classify the images. I will use the Dermnet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, datasets\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import models\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "\n",
    "# load config\n",
    "with open('config/dermnet.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Check if GPU is available and if not, use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((config['input_size'], config['input_size'])),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((config['input_size'], config['input_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_ds = datasets.ImageFolder(config['train_dir'], transform=train_transforms)\n",
    "test_ds = datasets.ImageFolder(config['test_dir'], transform=test_transforms)\n",
    "\n",
    "# Create validation set\n",
    "val_size = int(config['val_split'] * len(train_ds))\n",
    "train_size = len(train_ds) - val_size\n",
    "train_ds, val_ds = random_split(train_ds, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_dl = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    dataset=val_ds,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_dl:\n",
    "    print('images.shape:', images.shape)\n",
    "    print('labels.shape:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "base_model = models.inception_v3(pretrained=True)\n",
    "\n",
    "# Freeze the parameters\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Get the number of input features for the classifier\n",
    "num_features = base_model.fc.in_features \n",
    "\n",
    "# Define the classifier\n",
    "class DermNetClassifier(nn.Module):\n",
    "    \"\"\"DermNet classifier\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DermNetClassifier, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_features, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, config['num_classes'])\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=1024)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)  # flatten tensor\n",
    "        x = self.dropout( F.relu( self.bn1( self.fc1(x) ) ) )\n",
    "        x = self.dropout( F.relu( self.bn2( self.fc2(x) ) ) )\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "    \n",
    "# Replace the classifier in the pre-trained model with our classifier\n",
    "base_model.fc = DermNetClassifier()\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(base_model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Move model to GPU if available\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "# log training\n",
    "writer = SummaryWriter('logs/dermnet')\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    # Progress bar\n",
    "    progress_bar = tqdm(total=len(train_dl.dataset), desc=f'Epoch {epoch + 1}')\n",
    "\n",
    "    # Training\n",
    "    base_model.train()\n",
    "    for inputs, labels in train_dl:\n",
    "        # Move tensors to GPU if CUDA is available\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, _ = base_model(inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add loss to the training set's running loss\n",
    "        train_loss += loss.item() * inputs.size()[0]\n",
    "\n",
    "        # Calculate accuracy\n",
    "        top_p, top_class = outputs.topk(1, dim=1)\n",
    "        equals = (top_class == labels.view(*top_class.shape))\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "        # Log loss and accuracy\n",
    "        history['train_loss'].append(loss.item()/len(inputs))\n",
    "        history['train_acc'].append(accuracy/len(inputs))\n",
    "        writer.add_scalar('Loss/train', loss.item()/len(inputs), epoch)\n",
    "        writer.add_scalar('Accuracy/train', accuracy/len(inputs), epoch)\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'train_loss': '{:.3f}'.format(loss.item()/len(inputs)),\n",
    "            'train_acc': '{:.3f}'.format(accuracy/len(inputs))\n",
    "        })\n",
    "        progress_bar.update(inputs.shape[0])\n",
    "\n",
    "    # End of epoch - evaluate on validation set\n",
    "    with torch.no_grad():\n",
    "        base_model.eval()\n",
    "        for inputs, labels in val_dl:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = base_model(inputs)\n",
    "            valloss = criterion(outputs, labels)\n",
    "            val_loss += valloss.item() * inputs.size()[0]\n",
    "\n",
    "            top_p, top_class = outputs.topk(1, dim=1)\n",
    "            equals = (top_class == labels.view(*top_class.shape))\n",
    "            val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "    # Log loss and accuracy\n",
    "    history['val_loss'].append(val_loss/len(val_dl.dataset))\n",
    "    history['val_acc'].append(val_accuracy/len(val_dl.dataset))\n",
    "    writer.add_scalar('Loss/val', val_loss/len(val_dl.dataset), epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_accuracy/len(val_dl.dataset), epoch)\n",
    "\n",
    "    # Update progress bar    \n",
    "    progress_bar.set_postfix({\n",
    "        'val_loss': '{:.3f}'.format(val_loss/len(val_dl.dataset)),\n",
    "        'val_acc': '{:.3f}'.format(val_accuracy/len(val_dl.dataset))\n",
    "    })\n",
    "    progress_bar.close()\n",
    "    \n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
