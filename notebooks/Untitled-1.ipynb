{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Colab Notebook Set Up\n",
    "\n",
    "Use this cell to upload your kaggle.json file as well as the `download_data.sh`\n",
    "and `preprocess.py` scripts.\n",
    "\"\"\"\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Upload files\n",
    "kaggle = files.upload()\n",
    "data_script = files.upload()\n",
    "preprocess_script = files.upload()\n",
    "config = files.upload()\n",
    "\n",
    "# Verify uploads\n",
    "for file in [\"kaggle.json\", \"download_data.sh\", \"preprocess.py\", \"oct.yaml\"]:\n",
    "    assert file in os.listdir(), f\"Make sure you upload the {file} file\"\n",
    "\n",
    "# Shell commands\n",
    "!mkdir -p ~/.kaggle/ data/ models/ config/ scripts/\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!mv download_data.sh preprocess.py scripts/\n",
    "!mv oct.yaml config/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!chmod +x scripts/download_data.sh scripts/preprocess.py\n",
    "!pip install -q kaggle pretrainedmodels rich\n",
    "\n",
    "# Run shell commands\n",
    "!scripts/download_data.sh\n",
    "!python scripts/preprocess.py --config config/oct.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "import pretrainedmodels as models\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from rich import print\n",
    "from glob import glob\n",
    "\n",
    "import sys; sys.path.append(\".\")\n",
    "from net import train\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "import os\n",
    "import re\n",
    "\n",
    "config = 'config/oct.yaml'\n",
    "\n",
    "with open(config, \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muaddib/.conda/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/muaddib/.conda/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/home/muaddib/.conda/envs/torch/lib/python3.10/site-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class FineTuned(nn.Module):\n",
    "    \"\"\"\n",
    "    Fine-tuned Output Layer for InceptionV3\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(FineTuned, self).__init__()\n",
    "\n",
    "        # self.ptm = models.__dict__[config[\"model-name\"]](num_classes=1000, pretrained=\"imagenet\")\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, config['num-classes']),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_ds = train.FeatureDataset(config['features']['train'])\n",
    "test_ds  = train.FeatureDataset(config['features']['test'])\n",
    "val_ds   = train.FeatureDataset(config['features']['val'])\n",
    "\n",
    "# Create dataloaders\n",
    "train_dl = DataLoader(train_ds, batch_size=config['batch-size'], shuffle=True)\n",
    "test_dl  = DataLoader(test_ds, batch_size=config['batch-size'], shuffle=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=config['batch-size'], shuffle=False)\n",
    "\n",
    "# Instantiate model\n",
    "model = FineTuned(config)\n",
    "model.to(train.device())\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_idx, writer):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_dl):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(train.device())\n",
    "        labels = labels.to(train.device())\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss and its gradient\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather and report statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:  # Print every 10 mini-batches\n",
    "            last_loss = running_loss / 10  # loss per batch\n",
    "            print(f'    batch {i+1} loss: {last_loss:.3f}')\n",
    "            \n",
    "            tb_x = epoch_idx * len(train_dl) + i + 1\n",
    "            writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(val_dl):\n",
    "        vinputs, vlabels = vdata\n",
    "        vinputs = vinputs.to(train.device())\n",
    "        vlabels = vlabels.to(train.device())\n",
    "\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = criterion(voutputs, vlabels)\n",
    "        running_vloss += vloss.item()\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch for both training and validation\n",
    "    writer.add_scalars(\n",
    "        'Training vs. Validation Loss',\n",
    "        { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "        epoch_number + 1\n",
    "    )\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance:\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = f'models/oct_cnn_{timestamp}_{epoch_number}.pth'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_idx, writer):\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for i, data in enumerate(train_dl):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(train.device())\n",
    "        labels = labels.to(train.device())\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss and its gradient\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather and report statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        if i % 20 == 19:  # Print every 10 mini-batches\n",
    "            accuracy = correct_predictions / total_predictions\n",
    "            last_loss = running_loss / 10  # loss per batch\n",
    "            print(f'    batch {i+1} loss: {last_loss:.3f}, accuracy: {accuracy:.3f}')\n",
    "\n",
    "            tb_x = epoch_idx * len(train_dl) + i + 1\n",
    "            writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            writer.add_scalar('Accuracy/train', accuracy, tb_x)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    return last_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss, avg_acc = train_one_epoch(epoch_number, writer)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for i, vdata in enumerate(val_dl):\n",
    "        vinputs, vlabels = vdata\n",
    "        vinputs = vinputs.to(train.device())\n",
    "        vlabels = vlabels.to(train.device())\n",
    "\n",
    "        voutputs = model(vinputs)\n",
    "        vloss = criterion(voutputs, vlabels)\n",
    "        running_vloss += vloss.item()\n",
    "\n",
    "        _, predicted = torch.max(voutputs.data, 1)\n",
    "        total_predictions += vlabels.size(0)\n",
    "        correct_predictions += (predicted == vlabels).sum().item()\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    avg_vacc = correct_predictions / total_predictions\n",
    "    print('LOSS train {} valid {}. ACCURACY train {} valid {}'.format(avg_loss, avg_vloss, avg_acc, avg_vacc))\n",
    "\n",
    "    # Log the running loss averaged per batch for both training and validation\n",
    "    writer.add_scalars(\n",
    "        'Training vs. Validation Loss',\n",
    "        { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "        epoch_number + 1\n",
    "    )\n",
    "    writer.add_scalars(\n",
    "        'Training vs. Validation Accuracy',\n",
    "        { 'Training' : avg_acc, 'Validation' : avg_vacc },\n",
    "        epoch_number + 1\n",
    "    )\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance:\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = f'models/oct_cnn_{timestamp}_{epoch_number}.pth'\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    pbar = train.pbar(train_dl)\n",
    "\n",
    "    for i, (inputs, labels) in pbar:\n",
    "        inputs = inputs.to(train.device())\n",
    "        labels = labels.to(train.device())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        acc = train.accuracy(outputs, labels)\n",
    "        running_acc += acc\n",
    "\n",
    "        pbar.set_description(f\"Epoch [{epoch + 1}/{config['epochs']}]\")\n",
    "        pbar.set_postfix(loss=loss.item(), accuracy=acc)\n",
    "\n",
    "    train_loss = running_loss / len(train_dl)\n",
    "    train_losses.append(train_loss)\n",
    "    train_acc = running_acc / len(train_dl)\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = train.pbar(val_dl)\n",
    "\n",
    "        for i, (inputs, labels) in pbar:\n",
    "            inputs = inputs.to(train.device())\n",
    "            labels = labels.to(train.device())\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            batch_loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += batch_loss.item()\n",
    "            acc = train.accuracy(outputs, labels)\n",
    "            val_acc += acc\n",
    "\n",
    "            pbar.set_description(f\"Epoch [{epoch + 1}/{config['epochs']}]\")\n",
    "            pbar.set_postfix(loss=batch_loss.item(), accuracy=acc)\n",
    "\n",
    "    val_loss /= len(val_dl)\n",
    "    val_losses.append(val_loss)\n",
    "    val_acc /= len(val_dl)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{config['epochs']}.. \"\n",
    "          f\"Train loss: {train_loss:.3f}.. \"\n",
    "          f\"Train accuracy: {train_acc:.3f}.. \"\n",
    "          f\"Validation loss: {val_loss:.3f}.. \"\n",
    "          f\"Validation accuracy: {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
