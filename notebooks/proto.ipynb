{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Colab Notebook Set Up\n",
    "\n",
    "Use this cell to upload your kaggle.json file as well as the `download_data.sh`\n",
    "and `preprocess.py` scripts.\n",
    "\"\"\"\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Upload files\n",
    "kaggle = files.upload()\n",
    "data_script = files.upload()\n",
    "preprocess_script = files.upload()\n",
    "config = files.upload()\n",
    "\n",
    "# Verify uploads\n",
    "for file in [\"kaggle.json\", \"download_data.sh\", \"preprocess.py\", \"oct.yaml\"]:\n",
    "    assert file in os.listdir(), f\"Make sure you upload the {file} file\"\n",
    "\n",
    "# Shell commands\n",
    "!mkdir -p ~/.kaggle/ data/ models/ config/ scripts/\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!mv download_data.sh preprocess.py scripts/\n",
    "!mv oct.yaml config/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!chmod +x scripts/download_data.sh scripts/preprocess.py\n",
    "!pip install -q kaggle pretrainedmodels rich\n",
    "\n",
    "!sed -i -e 's/\\r$//' scripts/download_data.sh\n",
    "\n",
    "# Run shell commands\n",
    "!scripts/download_data.sh\n",
    "!python scripts/preprocess.py --config config/oct.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "import pretrainedmodels as models\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from rich import print\n",
    "from glob import glob\n",
    "\n",
    "import sys; sys.path.append(\".\")\n",
    "from net import train\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import time\n",
    "import yaml\n",
    "import os\n",
    "import re\n",
    "\n",
    "config = 'config/oct.yaml'\n",
    "\n",
    "with open(config, \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuned(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FineTuned, self).__init__()\n",
    "\n",
    "        self.model = models.__dict__[config[\"model-name\"]](num_classes=1000, pretrained=\"imagenet\")\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.model.last_linear = nn.Linear(self.model.last_linear.in_features, config[\"num-classes\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "# Create dataloaders\n",
    "loaders = train.feature_dataloaders(config)\n",
    "\n",
    "# Create model and move to GPU\n",
    "model = FineTuned(config)\n",
    "model = model.to(train.device())\n",
    "\n",
    "# Define parameters to optimize\n",
    "update_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(update_params, lr=config[\"lr\"])\n",
    "# optimizer = optim.SGD(update_params, lr=config[\"lr\"], momentum=config[\"momentum\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    dataloaders,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=25,\n",
    "    scheduler=None,\n",
    "    writer=None,\n",
    "    verbose=True,\n",
    "    is_inception=True,\n",
    "):\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in tqdm(dataloaders[phase], leave=False):\n",
    "                inputs = inputs.to(train.device())\n",
    "                labels = labels.to(train.device())\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \"\"\"\n",
    "                    Inception is a special case during training because it has an auxiliary output.\n",
    "                    When training, we calculate the loss by summing the final output and the auxiliary output\n",
    "                    but during testing we only consider the final output.\n",
    "                    \"\"\"\n",
    "                    if is_inception and phase == 'train':\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
